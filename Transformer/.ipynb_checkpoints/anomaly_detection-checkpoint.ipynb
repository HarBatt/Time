{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685b221",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeea6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from shared.component_logger import component_logger as logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a812411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "np.random.seed(3)\n",
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63928878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedd(nn.Module):\n",
    "    def __init__(self, in_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696bdc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim//n_heads\n",
    "        self.scale = self.head_dim** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "\n",
    "        # Sanity check\n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        #(n_samples, seq_len + 1, 3 * dim)\n",
    "        qkv = self.qkv(x)  \n",
    "        \n",
    "        #(n_smaples, seq_len + 1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        \n",
    "        #(3, n_samples, n_heads, seq_len + 1, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  \n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        #(n_samples, n_heads, head_dim, seq_len + 1)\n",
    "        k_t = k.transpose(-2, -1)  \n",
    "        \n",
    "        # (n_samples, n_heads, seq_len + 1, seq_len + 1)\n",
    "        dp = (q @ k_t)*self.scale \n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, seq_len + 1, seq_len + 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        # (n_samples, n_heads, seq_len +1, head_dim)\n",
    "        weighted_avg = attn @ v  \n",
    "        \n",
    "        # (n_samples, seq_len + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)  \n",
    "        \n",
    "        # (n_samples, seq_len + 1, dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  \n",
    "        \n",
    "        # (n_samples, seq_len + 1, dim)\n",
    "        x = self.proj(weighted_avg)  \n",
    "        \n",
    "        # (n_samples, seq_len + 1, dim)\n",
    "        x = self.proj_drop(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbd7090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # (n_samples, seq_len + 1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, seq_len + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, seq_len + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, seq_len + 1, out_features)\n",
    "        x = self.drop(x)  # (n_samples, seq_len + 1, out_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e21b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=hidden_features, out_features=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_norm = self.norm1(x)\n",
    "        out_attn = self.attn(out_norm)\n",
    "        x = x + out_attn\n",
    "\n",
    "        out_norm = self.norm2(x)\n",
    "        out_mlp = self.mlp(out_norm)\n",
    "        x = x + out_mlp\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ca9f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(nn.Module):\n",
    "    def __init__(self, in_dim, seq_len, embed_dim, out_dim, depth=12, n_heads=12, mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.,):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedd(in_dim, embed_dim)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # Total number of tokens = 1 + seq_len\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + seq_len, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim,\n",
    "                n_heads=n_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                p=p,\n",
    "                attn_p=attn_p,\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "            ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear(embed_dim, out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1)  # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + seq_len, embed_dim)\n",
    "\n",
    "        # Added positional embedding of the cls token + all the patches to indicate the positions. \n",
    "        x = x + self.pos_embed  # (n_samples, 1 + seq_len, embed_dim)\n",
    "        x = self.pos_drop(x) # (n_samples, 1 + seq_len, embed_dim) (probability of dropping)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "\n",
    "        return x, cls_token_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bd958",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5358bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(data_frame, down_len, labels):\n",
    "    np_data = np.array(data_frame)\n",
    "    orig_len, col_num = np_data.shape\n",
    "    down_time_len = orig_len // down_len # integer division to get the number of downsampled time steps\n",
    "    np_data = np_data.transpose()\n",
    "    d_data = np_data[:, :down_time_len*down_len].reshape(col_num, -1, down_len) # reshape the data into a 3D array\n",
    "    d_data = np.median(d_data, axis=2).reshape(col_num, -1) # take the median of the downsampled data to reduce the size\n",
    "    d_data = d_data.transpose()\n",
    "    if labels is not None:\n",
    "        np_labels = np.array(labels)\n",
    "        d_labels = np_labels[:down_time_len*down_len].reshape(-1, down_len)\n",
    "        d_labels = np.round(np.max(d_labels, axis=1))\n",
    "\n",
    "    else:\n",
    "        d_labels = None\n",
    "\n",
    "    return d_data, d_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4778c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, csv_file, seq_len):\n",
    "        self.file = csv_file\n",
    "        self.file = self.file.drop(columns = [\"Unnamed: 0\", \"Timestamp\"])\n",
    "        self.file = self.file.replace(np.nan, self.file.mean())\n",
    "        self.attack = self.file[\"attack\"]\n",
    "        self.file = self.file.drop(columns = [\"attack\"])\n",
    "        self.file, self.attack = downsample(self.file, 10, self.attack)\n",
    "        self.data = self.file\n",
    "        ori_data = self.data.copy()\n",
    "        # Normalize the data\n",
    "        ori_data, (minimum, maximum) = self.MinMaxScaler(ori_data)\n",
    "        temp_data = [] \n",
    "        # Cut data by sequence length\n",
    "        for i in range(seq_len + 1, len(ori_data)):\n",
    "            x = ori_data[i-seq_len:i]\n",
    "            y = ori_data[i].reshape(1, -1)\n",
    "            target = self.attack[i].reshape(1, -1)\n",
    "            temp_data.append((x, y, target))\n",
    "        self.temp_array = temp_data\n",
    "    \n",
    "    def MinMaxScaler(self, data):\n",
    "        minimum, maximum = np.min(data, 0), np.max(data, 0)\n",
    "        numerator = data - minimum\n",
    "        denominator = maximum - minimum\n",
    "        norm_data = numerator / (denominator + 1e-7)\n",
    "        return norm_data, (minimum, maximum)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.temp_array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.temp_array[index][0])\n",
    "        y = torch.Tensor(self.temp_array[index][1])\n",
    "        label = torch.Tensor(self.temp_array[index][2])\n",
    "        return (x, y, label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c08066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>MV101</th>\n",
       "      <th>P101</th>\n",
       "      <th>P102</th>\n",
       "      <th>AIT201</th>\n",
       "      <th>AIT202</th>\n",
       "      <th>AIT203</th>\n",
       "      <th>...</th>\n",
       "      <th>P501</th>\n",
       "      <th>P502</th>\n",
       "      <th>PIT501</th>\n",
       "      <th>PIT502</th>\n",
       "      <th>PIT503</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>P601</th>\n",
       "      <th>P602</th>\n",
       "      <th>P603</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28/12/2015 10:00:00 AM</td>\n",
       "      <td>2.427057</td>\n",
       "      <td>522.8467</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>262.0161</td>\n",
       "      <td>8.396437</td>\n",
       "      <td>328.6337</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250.8652</td>\n",
       "      <td>1.649953</td>\n",
       "      <td>189.5988</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28/12/2015 10:00:01 AM</td>\n",
       "      <td>2.446274</td>\n",
       "      <td>522.8860</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>262.0161</td>\n",
       "      <td>8.396437</td>\n",
       "      <td>328.6337</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250.8652</td>\n",
       "      <td>1.649953</td>\n",
       "      <td>189.6789</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>28/12/2015 10:00:02 AM</td>\n",
       "      <td>2.489191</td>\n",
       "      <td>522.8467</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>262.0161</td>\n",
       "      <td>8.394514</td>\n",
       "      <td>328.6337</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250.8812</td>\n",
       "      <td>1.649953</td>\n",
       "      <td>189.6789</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28/12/2015 10:00:03 AM</td>\n",
       "      <td>2.534350</td>\n",
       "      <td>522.9645</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>262.0161</td>\n",
       "      <td>8.394514</td>\n",
       "      <td>328.6337</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250.8812</td>\n",
       "      <td>1.649953</td>\n",
       "      <td>189.6148</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28/12/2015 10:00:04 AM</td>\n",
       "      <td>2.569260</td>\n",
       "      <td>523.4748</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>262.0161</td>\n",
       "      <td>8.394514</td>\n",
       "      <td>328.6337</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250.8812</td>\n",
       "      <td>1.649953</td>\n",
       "      <td>189.5027</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                Timestamp    FIT101    LIT101   MV101  P101  \\\n",
       "0           0   28/12/2015 10:00:00 AM  2.427057  522.8467       2     2   \n",
       "1           1   28/12/2015 10:00:01 AM  2.446274  522.8860       2     2   \n",
       "2           2   28/12/2015 10:00:02 AM  2.489191  522.8467       2     2   \n",
       "3           3   28/12/2015 10:00:03 AM  2.534350  522.9645       2     2   \n",
       "4           4   28/12/2015 10:00:04 AM  2.569260  523.4748       2     2   \n",
       "\n",
       "   P102    AIT201    AIT202    AIT203  ...  P501  P502    PIT501    PIT502  \\\n",
       "0     1  262.0161  8.396437  328.6337  ...     2     1  250.8652  1.649953   \n",
       "1     1  262.0161  8.396437  328.6337  ...     2     1  250.8652  1.649953   \n",
       "2     1  262.0161  8.394514  328.6337  ...     2     1  250.8812  1.649953   \n",
       "3     1  262.0161  8.394514  328.6337  ...     2     1  250.8812  1.649953   \n",
       "4     1  262.0161  8.394514  328.6337  ...     2     1  250.8812  1.649953   \n",
       "\n",
       "     PIT503    FIT601  P601  P602  P603  attack  \n",
       "0  189.5988  0.000128     1     1     1       0  \n",
       "1  189.6789  0.000128     1     1     1       0  \n",
       "2  189.6789  0.000128     1     1     1       0  \n",
       "3  189.6148  0.000128     1     1     1       0  \n",
       "4  189.5027  0.000128     1     1     1       0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"swat\"\n",
    "train_path = \"data/\" + data + \"_train_n.csv\" \n",
    "test_path = \"data/\" + data + \"_test_n.csv\" \n",
    "model_path = \"saved_model/\" + data + \"_time_transformer.pt\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae40f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 1\n",
    "seq_len = 16\n",
    "batch_size = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print_every = 100 # 2000 minibatches\n",
    "epochs = 25\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "868ecbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = TimeDataset(train_df, seq_len)\n",
    "testdataset = TimeDataset(test_df, seq_len)\n",
    "trainloader = DataLoader(dataset=traindataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f462fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = {\n",
    "        \"in_dim\": 51,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"embed_dim\": 16,\n",
    "        \"depth\": 24,\n",
    "        \"n_heads\": 16,\n",
    "        \"qkv_bias\": True,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"out_dim\": 51\n",
    "}\n",
    "net = TimeTransformer(**custom_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f96da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 18:53:05.130757: INFO: time: <cell line: 5>: Number of trainable parameters: 80739\n"
     ]
    }
   ],
   "source": [
    "def get_parameters_count(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "logger.log(\"Number of trainable parameters: {}\".format(get_parameters_count(net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5102b3",
   "metadata": {},
   "source": [
    "### Define a Loss function and optimizer\n",
    "\n",
    "Let’s use a MSE loss and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae3ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f70f13",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f9a8041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-23 18:54:19.971562: INFO: time: <cell line: 1>: Epoch: 1; mse_train: 0.10319\n",
      "2022-07-23 18:55:35.054251: INFO: time: <cell line: 1>: Epoch: 2; mse_train: 0.02252\n",
      "2022-07-23 18:56:49.454319: INFO: time: <cell line: 1>: Epoch: 3; mse_train: 0.01585\n",
      "2022-07-23 18:58:03.608165: INFO: time: <cell line: 1>: Epoch: 4; mse_train: 0.01177\n",
      "2022-07-23 18:59:17.388446: INFO: time: <cell line: 1>: Epoch: 5; mse_train: 0.00769\n",
      "2022-07-23 19:00:30.727357: INFO: time: <cell line: 1>: Epoch: 6; mse_train: 0.00533\n",
      "2022-07-23 19:01:44.329556: INFO: time: <cell line: 1>: Epoch: 7; mse_train: 0.00439\n",
      "2022-07-23 19:02:58.098986: INFO: time: <cell line: 1>: Epoch: 8; mse_train: 0.00374\n",
      "2022-07-23 19:04:12.314006: INFO: time: <cell line: 1>: Epoch: 9; mse_train: 0.00329\n",
      "2022-07-23 19:05:27.073742: INFO: time: <cell line: 1>: Epoch: 10; mse_train: 0.00294\n",
      "2022-07-23 19:06:42.600848: INFO: time: <cell line: 1>: Epoch: 11; mse_train: 0.00268\n",
      "2022-07-23 19:07:56.446465: INFO: time: <cell line: 1>: Epoch: 12; mse_train: 0.00252\n",
      "2022-07-23 19:09:09.881937: INFO: time: <cell line: 1>: Epoch: 13; mse_train: 0.00237\n",
      "2022-07-23 19:10:21.587154: INFO: time: <cell line: 1>: Epoch: 14; mse_train: 0.00222\n",
      "2022-07-23 19:11:34.448609: INFO: time: <cell line: 1>: Epoch: 15; mse_train: 0.00212\n",
      "2022-07-23 19:12:48.181663: INFO: time: <cell line: 1>: Epoch: 16; mse_train: 0.00204\n",
      "2022-07-23 19:14:00.967270: INFO: time: <cell line: 1>: Epoch: 17; mse_train: 0.00197\n",
      "2022-07-23 19:15:14.366136: INFO: time: <cell line: 1>: Epoch: 18; mse_train: 0.00189\n",
      "2022-07-23 19:16:28.138675: INFO: time: <cell line: 1>: Epoch: 19; mse_train: 0.00184\n",
      "2022-07-23 19:17:43.278536: INFO: time: <cell line: 1>: Epoch: 20; mse_train: 0.00178\n",
      "2022-07-23 19:18:58.550523: INFO: time: <cell line: 1>: Epoch: 21; mse_train: 0.00172\n",
      "2022-07-23 19:20:14.489547: INFO: time: <cell line: 1>: Epoch: 22; mse_train: 0.00168\n",
      "2022-07-23 19:21:30.421686: INFO: time: <cell line: 1>: Epoch: 23; mse_train: 0.00162\n",
      "2022-07-23 19:22:44.489455: INFO: time: <cell line: 1>: Epoch: 24; mse_train: 0.00159\n",
      "2022-07-23 19:23:57.402207: INFO: time: <cell line: 1>: Epoch: 25; mse_train: 0.00154\n",
      "2022-07-23 19:23:57.406791: INFO: time: <cell line: 23>: Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, forecast, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        forecast = forecast.to(device)\n",
    "        forecast = torch.squeeze(forecast, dim = 1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, embeddings = net(inputs)\n",
    "        loss = criterion(outputs, forecast)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    if epoch%checkpoint == 0:\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(), 'train_loss': train_loss}, \n",
    "                   model_path)\n",
    "        logger.log(\"Epoch: {}; mse_train: {}\".format(epoch + 1, np.round(train_loss/len(trainloader), 5)))\n",
    "\n",
    "logger.log('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b58a9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(dataset=testdataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a07df5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    if i==1000: break\n",
    "    inputs, forecast, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    forecast = forecast.to(device)\n",
    "    forecast = torch.squeeze(forecast, dim = 1)\n",
    "    outputs, embeddings = net(inputs)\n",
    "    loss = criterion(outputs, forecast)\n",
    "    #print(loss)\n",
    "    if loss >= 0.03:\n",
    "        predictions.append(1.0)\n",
    "    else:\n",
    "        predictions.append(0.0)\n",
    "    #print(loss)\n",
    "    #print(labels.flatten())\n",
    "    ground_truth.append(labels.flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12e43ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4af71a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5457627118644067\n"
     ]
    }
   ],
   "source": [
    "precision = tp/(tp + fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "773a615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5649122807017544\n"
     ]
    }
   ],
   "source": [
    "recall = tp/(tp + fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dec16bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5551724137931033\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*(precision*recall)/(precision + recall)\n",
    "print(f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
